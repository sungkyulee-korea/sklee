#!/usr/bin/env python3
# .github/scripts/security_scan.py
import os, json, re, requests, sys, textwrap

# --- helpers ---
def load_json(path):
    try:
        with open(path, 'r', encoding='utf8') as f:
            return json.load(f)
    except Exception:
        return None

def write_json(path, obj):
    with open(path, 'w', encoding='utf8') as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)

def short(text, n=120000):
    if text is None:
        return ''
    return text if len(text) <= n else text[:n] + "\n\n[...truncated]"

# --- 1) Determine changed files (PR preferred) ---
def get_changed_files():
    evt_path = os.environ.get('GITHUB_EVENT_PATH')
    repo = os.environ.get('GITHUB_REPOSITORY','')
    files = []
    # If PR event, use GitHub API (via GITHUB_TOKEN)
    if evt_path and os.path.exists(evt_path):
        ev = json.load(open(evt_path, 'r', encoding='utf8'))
        pr = ev.get('pull_request')
        if pr and pr.get('number'):
            # Try to list PR files using REST via GITHUB_TOKEN (best)
            token = os.environ.get('GITHUB_TOKEN')
            if token and repo:
                owner, repo_name = repo.split('/')
                page = 1
                headers = {'Authorization': f'bearer {token}', 'Accept': 'application/vnd.github+json'}
                while True:
                    url = f'https://api.github.com/repos/{owner}/{repo_name}/pulls/{pr["number"]}/files?per_page=100&page={page}'
                    r = requests.get(url, headers=headers, timeout=30)
                    if r.status_code != 200:
                        break
                    arr = r.json()
                    if not arr:
                        break
                    for it in arr:
                        files.append(it.get('filename'))
                    if len(arr) < 100:
                        break
                    page += 1
    # fallback: try git diff before/after
    if not files and evt_path and os.path.exists(evt_path):
        try:
            ev = json.load(open(evt_path, 'r', encoding='utf8'))
            before = ev.get('before','')
            after = ev.get('after', os.environ.get('GITHUB_SHA',''))
            if before and after and before != after:
                import subprocess
                out = subprocess.check_output(['git','diff','--name-only', before, after]).decode().splitlines()
                files = out
        except Exception:
            files = []
    # fallback: list all tracked files
    if not files:
        try:
            import subprocess
            out = subprocess.check_output(['git','ls-files']).decode().splitlines()
            files = out
        except Exception:
            files = []
    # dedupe & filter
    seen = []
    for f in files:
        if f and f not in seen:
            seen.append(f)
    return seen[:200]

# --- 2) Read file contents truncated ---
def read_contents(files, max_bytes=150000):
    out = {}
    for f in files:
        try:
            if os.path.exists(f) and os.path.isfile(f):
                s = open(f,'r',encoding='utf8',errors='ignore').read()
                out[f] = short(s, max_bytes)
            else:
                out[f] = '[not available]'
        except Exception as e:
            out[f] = f'[error reading: {e}]'
    return out

# --- 3) Heuristic scan (regex patterns) ---
HEUR_PATTERNS = {
    "XSS_output_like": re.compile(r"response\.getWriter|getWriter\(|println\(", re.I),
    "XSS_replaceAll": re.compile(r"\.replaceAll\(", re.I),
    "SQL_concat_in_exec": re.compile(r"execute(Query|Update)?\(|execute\).*\+", re.I),
    "SQL_stmt": re.compile(r"\bnew\s+Statement\b|\bStatement\b", re.I),
    "LDAP_usage": re.compile(r"\b(InitialDirContext|DirContext|ldap|LDAP)\b", re.I),
}

def heuristic_scan(contents):
    out = {}
    for f, text in contents.items():
        hits = []
        try:
            for i, line in enumerate(text.splitlines(), start=1):
                for name, pat in HEUR_PATTERNS.items():
                    try:
                        if pat.search(line):
                            hits.append({"line": i, "pattern": name, "snippet": line.strip()[:300]})
                    except Exception:
                        pass
        except Exception as e:
            hits.append({"error": str(e)})
        out[f] = hits
    return out

# --- 4) Semgrep summary (short) ---
def semgrep_summary(semgrep_json):
    items = semgrep_json.get('results', []) if semgrep_json else []
    lines = []
    for r in items:
        path = r.get('path') or r.get('check_id') or '-'
        start = '-'
        try:
            st = r.get('start') or r.get('extra', {}).get('start') or {}
            if isinstance(st, dict):
                start = st.get('line','-')
        except Exception:
            start = '-'
        message = r.get('extra', {}).get('message') or r.get('message') or ''
        sev = r.get('extra', {}).get('severity') or ''
        lines.append(f"{path}:{start} [{sev}] {message}")
    return lines

# --- 5) (Optional) Call OpenAI to enrich findings ---
def call_openai_enrich(semgrep_lines, heur_text, files_block):
    key = os.environ.get('SKLEE_OPENAI_API_KEY') or os.environ.get('OPENAI_API_KEY')
    if not key:
        return None
    prompt = textwrap.dedent(f"""
    You are a concise security code reviewer. Given semgrep findings and heuristics, provide a JSON array of items with fields:
    file, line, severity (Low/Medium/High/Critical), cwe (CWE-xxx or -), owasp (Axx or -), present (true/false), details_ko, recommendation_ko.
    Semgrep lines:
    {'\\n'.join(semgrep_lines[:100])}

    Heuristics sample:
    {heur_text[:5000]}

    File contents (truncated):
    {files_block[:120000]}

    Return JSON only: {" + '"items":[{...}]}' + "}
    """)
    headers = {"Authorization": f"Bearer {key}", "Content-Type": "application/json"}
    body = {
        "model": "gpt-4o-mini",
        "messages": [
            {"role":"system","content":"You are a concise, security-focused code reviewer. Produce JSON only."},
            {"role":"user","content": prompt}
        ],
        "temperature": 0,
        "max_tokens": 1200
    }
    try:
        r = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=body, timeout=60)
        if r.status_code != 200:
            return {"error": f"OpenAI {r.status_code}", "raw": r.text[:1000]}
        text = r.json().get('choices',[{}])[0].get('message',{}).get('content','')
        m = re.search(r'(\{[\s\S]*\})\s*$', text.strip())
        if m:
            try:
                return json.loads(m.group(1))
            except Exception:
                return {"error":"parse_error","raw": text[:2000]}
        else:
            return {"error":"no_json","raw": text[:2000]}
    except Exception as e:
        return {"error": str(e)}

# --- main ---
def main():
    semgrep_json = load_json('semgrep-results.json') or {}
    files = get_changed_files()
    write_json('files_to_analyze.json', files)
    contents = read_contents(files)
    write_json('files_content.json', contents)

    heur = heuristic_scan(contents)
    write_json('heuristics.json', heur)

    sem_lines = semgrep_summary(semgrep_json)

    # build files_block for optional OpenAI
    files_block = ""
    for f in files:
        files_block += f"--- {f} ---\n{contents.get(f, '')}\n\n"

    # optional OpenAI enrichment (only if key present)
    ai_summary = None
    if os.environ.get('SKLEE_OPENAI_API_KEY') or os.environ.get('OPENAI_API_KEY'):
        ai_summary = call_openai_enrich(sem_lines, json.dumps(heur, ensure_ascii=False)[:5000], files_block)
        write_json('ai_summary_structured.json', ai_summary)
    else:
        write_json('ai_summary_structured.json', {"note":"OpenAI key not provided; enrichment skipped."})

    # Build report.md
    lines = []
    lines.append("### ğŸ¤– ìë™ ë³´ì•ˆ ë¦¬í¬íŠ¸")
    lines.append("")
    lines.append("#### 1) ë¶„ì„ ëŒ€ìƒ íŒŒì¼ (ìµœëŒ€ 200ê°œ)")
    lines.append("")
    for f in files:
        pr = "HIGH" if 'bad' in f.lower() else "normal"
        lines.append(f"- {f} ({pr})")
    lines.append("")
    lines.append("#### 2) Semgrep (ìš”ì•½)")
    lines.append("")
    if sem_lines:
        for l in sem_lines[:200]:
            lines.append(f"- {l}")
    else:
        lines.append("- Semgrep: ë¬¸ì œ ì—†ìŒ (ê¸°ë³¸ ë£° ê¸°ì¤€)")
    lines.append("")
    lines.append("#### 3) íœ´ë¦¬ìŠ¤í‹± ìƒ˜í”Œ")
    lines.append("")
    for f, hits in list(heur.items())[:200]:
        if hits:
            lines.append(f"- {f}:")
            for h in hits[:5]:
                if 'error' in h:
                    lines.append(f"  - ERROR: {h['error']}")
                else:
                    lines.append(f"  - L{h['line']} {h['pattern']} -> {h['snippet']}")
    lines.append("")
    lines.append("#### 4) OpenAI(ì„ íƒ) ìë™ ë¶„ì„ (ìˆë‹¤ë©´)")
    ai = load_json('ai_summary_structured.json') or {}
    lines.append("```json")
    lines.append(json.dumps(ai, ensure_ascii=False, indent=2))
    lines.append("```")
    lines.append("")
    lines.append("_ìë™ ë¦¬í¬íŠ¸ â€” ë‹´ë‹¹ì ê²€í†  í•„ìš”_")

    with open('report.md','w',encoding='utf8') as f:
        f.write("\n".join(lines))

    print("report.md created; files:", len(files))

if __name__ == '__main__':
    main()
